---
title: "Linear regression using R"
author: "Sean Walsh <sdwalsh335@gmail.com>"
date: "23 April 2020"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---

```{r setup, include = FALSE}
# Set global options for code chunks and load required R packages and data
library(dplyr)
library(ggplot2)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
data("mtcars")
```

## Simple Linear Regression

### Introduction

**Ordinary Least Squares (OLS) linear regression**, first developed by the French mathematician Adrien-Marie Legendre in 1805, is a statistical technique used for the analysis and modelling of a linear relationship between a numeric dependent variable and one or more independent variables (which can be continuous or categorical). Put simply, if the relationship between two variables is approximately linear, then a straight line can be fit to the data in order to model the relationship. Linear regression is used for both prediction and explanation. The regression equation (equation of a straight line) using one covariate takes the following form

$$
y = mx + c
$$

Where $y$ is the *dependent* variable, $m$ is the *gradient* of the line, $x$ is the value of the *independent* variable and $c$ is the *intercept* on the y-axis.

The formula can also be written more formally as

$$
y_i = \beta_{0} + \beta_{1}x_{1} + e_i
$$

where $\beta_{0}$ is the intercept, $\beta_{1}$ is the slope of the line, or coefficient, and $e$ is the error associated with the estimate of $y$.

**Multiple linear regression** is an extension of simple linear regression where additional covariates are accommodated

$$
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3} + ... + \beta_{p}x_{p} + e 
$$

The line of best fit is calculated in R using the base `lm()` function which also outputs the slope and intercept coefficients. The slope and intercept of a simple linear regression can also be calculated from five summary statistics: the standard deviations `sd()` of x and y, the means `mean()` of x and y, and the **Pearson correlation coefficient** `cor()` of x and y.

```{r slope and intercept, eval = FALSE}
# How to calculate the slope in linear regression
slope <- cor(x, y) * (sd(y) / sd(x))

# How to calculate the intercept in linear regression
intercept <- mean(y) - (slope * mean(x))
```

The **model assumptions** associated with the use of linear regression are:
  
1. A linear relationship between the dependent variable and the covariate(s)
2. Residuals/errors of fit are normally distributed (less important as sample size increases)
3. No/little multicollinearity (which affects significance tests)
4. Homoscedasticity (constant variance in residuals/errors of fit)
5. Outliers do not negatively impact the regression

### Exploration

The first model assumption can be quickly checked by plotting the data to see what type of relationship exists between the variables of interest. We will use the built-in `mtcars` dataset in R and visualise the relationship between fuel efficiency `mpg` and engine displacement `disp`.

```{r mpg vs disp}
# Scatterplot of mpg vs. disp
mtcars %>%
  ggplot(aes(x = disp,
             y = mpg)) +
  geom_point()
```

Upon visual inspection, the relationship is approximately linear, has a negative direction, and looks to be fairly strong. The strength of the direction can be quantified using the Pearson correlation coefficient

```{r corr 1}
# Pearson correlation coefficient
cor(mtcars$disp, 
    mtcars$mpg)

# Order of arguments does not matter
cor(mtcars$mpg,
    mtcars$disp)
```

Note that *correlation does not imply causation*; it just allows one to quantify associations. To determine causality, or at least provide evidence which supports the notion, one must typically carry out a well designed experiment (think randomised controlled trial).

If the relationship is non-linear, and linear regression is desired, a common approach is to transform the response variable and(or) predictor variable in order to coerce the relationship to one that approaches linearity. Common transformations include natural `log()`, base ten `log10()`, square root `sqrt()`, cube root and inverse. The `mpg` and `disp` relationship is already approximately linear but it can be strengthened using a square root transformation.

```{r sqrt mpg vs disp}
# Scatterplot of mpg vs. sqrt(disp)
mtcars %>%
  ggplot(aes(x = sqrt(disp), 
             y = mpg)) +
  geom_point()
```

Visually, there is not much difference but if we get the correlation once more

```{r corr 2}
# Pearson correlation coefficient
cor(sqrt(mtcars$disp), 
    mtcars$mpg)
```

### Modelling

The next step is to fit a linear regression model to the (transformed) data

```{r linear model fit}
# Fit the linear model using the formula method
linear_fit <- lm(mpg ~ sqrt(disp), data = mtcars)

# Get the model output
summary(linear_fit)
```

The model output above gives a five-number summary of the residual distribution. The goodness of fit can be quantified using the **residual standard error (RSE)**, which rarely differs from the root mean squared error, and the $r^2$ metric. The RSE quantifies the variance of the model errors and is an absolute measure of fit with units identical to the response variable. 

The $r^2$ metric for a simple linear regression is the Pearson correlation coefficient, $r$, squared and quantifies the proportion of variance explained by the model. It is important to consider $r^2$ when using a linear regression model for prediction.

What is the difference between the adjusted and multiple $r^2$ metrics given in the output? Adjusted $r^2$ should be used in multiple linear regression as it penalises for additional predictors which can increase $r^2$ even though there may be little association with the response variable.

Looking at the model coefficients, we see both the intercept and engine displacement coefficient estimates have highly significant $p$-values. The $t$-value is a measure of a coefficient's "signal" strength as it is the estimate divided by its corresponding standard error. A high absolute $t$-value corresponds to a strong predictor with the sign indicating direction of effect. Data scientists are often more concerned with $t$-values than $p$-values when selecting variables for predictive modelling.

Note that there is also a global $p$-value which is highly significant. This $p$-value represents the statistical significance of the overall regression. There may be cases where insignificant covariates are present in a model but the overall regression is still statistically significant.

```{r linear model plot}
# r-squared
r2 <- paste("r^2 == ", 0.78)

# Plot the data and the simple linear regression model
mtcars %>%
  ggplot(aes(x = sqrt(disp), 
             y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm") + # Leave 95% CI turned on
  annotate("text",
           label = "p < 0.001",
           x = 10,
           y = 12.5) +
  annotate("text",
           label = r2,
           parse = T,
           x = 10,
           y = 11)
```

The variance about the line of best fit looks okay. We can now check model assumptions 2 and 4. As we are dealing with just one covariate in simple linear regression, we do not need to check model assumption 3, multicollinearity. If it were a multiple linear regression, then we would use the `cor()` function and a threshold like 0.8 for the collinear cut-off. We could also use the **variance inflation factor (VIF)** to identify the presence of collinearity. Outliers do not seem to be a problem in these data so we do not give model assumption 5 any further consideration.

Passing the linear model to the `plot()` function returns a series of model diagnostic plots. The first and third plots allow us to check the homoscedasticity assumption (constant variance of residuals); no "fan" pattern is obvious in either plot. The second plot allows us to check the normality of residual errors; the points are generally close to the diagonal so this assumption is also met.

We do not need to investigate the final plot in the series but note that it does contain useful information on the effect of any outliers that may be present.

```{r diagnostics1}
# Model diagnostics
par(mfrow = c(2, 2))
plot(linear_fit)
```

What can we do if heteroscedasticity is present and we are worried that the regression might lead to unstable predictions? The model can be rebuilt using different covariates which can also be transformations of the originals. Another option is to use a **Box-Cox** transformation to coerce the dependent variable to an approximately normal distribution

```{r boxcox1}
# Box-Cox transformation
mpgBC <- BoxCoxTrans(mtcars$mpg)
```

Rebuild the model with the Box-Cox transformed mpg variable as the new response

```{r boxcox2}
# Attach to dataframe
mtcars <- cbind(mtcars, 
                mpg_BC = predict(mpgBC, newdata = mtcars$mpg))

# Recreate the model
lmMod_bc <- lm(mpg_BC ~ sqrt(disp), data = mtcars)

# Output
summary(lmMod_bc)
```

The multiple $r^2$ is greater than in the first model. Model diagnostics can be carried out in the same manner 

```{r boxcox3}
# Diagnostics
par(mfrow = c(2, 2))
plot(lmMod_bc)
```

Back to the original linear model. The coefficients 

```{r coeff 1}
# Get the intercept and slope
linear_fit$coefficients
```

should be identical to

```{r coeff 2}
# Formulae from earlier
slope <- cor(sqrt(mtcars$disp), mtcars$mpg) * (sd(mtcars$mpg) / sd(sqrt(mtcars$disp)))
intercept <- mean(mtcars$mpg) - (slope * mean(sqrt(mtcars$disp)))

slope; intercept
```

### Model interpretation

Using the first simple model, the $p$-value for engine displacement is highly significant and $r^2$, a measure of predictive power, is strong at 0.78. The take home message from the output is that for every unit increase in the square root of engine displacement there is a -1.28 decrease in fuel efficiency. Therefore, we infer from the model that fuel efficiency decreases with increasing engine displacement.

### Further considerations

The number of data points is important and can influence the $p$-values in the model. A rule of thumb for OLS linear regression is that *at least 20 data points* are required for a valid model. The $p$-value is the probability of observing data as (or more) extreme as those under investigation given a null distribution (i.e. the null hypothesis that there is no relationship between x and y, a coefficient is equal to zero, etc.). Statistical inference and $p$-value calculation will be covered in future meet-ups.

You may come across suggestions to standardise the covariates in regression models. This involves mapping the same scale ($z$-scores) to each covariate and results in the mean value of each covariate being 0. This can be useful depending on how the model is to be interpreted and communicated. When interpreting the intercept, it is described as the expected value of y when all standardised covariates are at their mean value (zero). Without standardisation, it is the expected value of y when x is equal to the absolute value of 0. Standardising is not bad or wrong, it is just unnecessary most of the time.

Sometimes the literal translation of the intercept is nonsensical but we ignore it in most cases. Statistical significance is also not always important (this just gives us the probability that the coefficient estimate is zero). For example, the expected value of mpg when engine displacement is zero is 38.8. Obviously this makes no sense as there would be no engine to measure fuel efficiency from if the displacement was zero cubic inches. The smallest engine displacement award actually goes to the *Tata Nano* (disp = 38 cubic inches).

In our simple linear regression model, we have a statistically significant intercept coefficient. What does this mean? It means that the intercept value is significantly different to zero. However, as mentioned earlier it cannot be taken literally as the expected mpg given an engine displacement of zero. We just use it as a constant that is added to or subtracted from as necessary given a vector of linear predictors.


## Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression. Additional covariates are added as necessary to make a linear model (i.e. each coefficient is linearly related to its variable). Let's add some more variables to our simple model from earlier.

```{r multi-regression}
# Create a multiple linear regression model
m_linear_fit <- lm(mpg ~ sqrt(disp) + wt + cyl, data = mtcars)

# Model summary
summary(m_linear_fit)
```

Engine displacement becomes insignificant when vehicle weight and the number of cylinders are included as covariates. Multicollinearity is at play here as all three of these covariates are strongly correlated

```{r correlation matrix}
# Create the correlation matrix
cor(mtcars[c(2, 3, 6)])
```

Remove engine displacement and update the model

```{r final model}
# Create a multiple linear regression model using 0.8 as a collinear cut-off
# Assess RMSE
final_linear_fit <- lm(mpg ~ cyl + wt, data = mtcars)

# Model summary
summary(final_linear_fit)
```

Now all model coefficients are statistically significant and the RSE and adjusted $r^2$ are also better than the simple model from earlier. Final model diagnostics can be carried using the `plot()` function once more

```{r diagnostics2}
# Model diagnostics
par(mfrow = c(2, 2))
plot(final_linear_fit)
```

## Prediction 

Making predictions with a model is easy in R. The `predict()` function takes as arguments the model object and a dataframe of observations to predict. The following example uses the multiple linear regression model above to predict the fuel efficiency of each vehicle in the `mtcars` dataset. 

```{r prediction1}
# Get predictions
predict(final_linear_fit, newdata = mtcars)
```

This is the same as reading the values of the fitted line. However, in practice, we would partition an unseen *test* set and *train* a model on the remaining data. The model would then be used to predict the unseen test set observations and a sensible performance metric would be assessed to determine model generalisability. **Cross-validation** is perhaps the best strategy to use when training, tuning and validating predictive models although parameter tuning is not a consideration when performing linear regression. Applied predictive modelling is another topic that will be covered in future meet-ups.

```{r prediction2}
# Partition train and test sets
set.seed(1)
data_part <- createDataPartition(y = mtcars$mpg, 
                                 p = 0.7, 
                                 list = F)
training <- mtcars[data_part, ]
test <- mtcars[-data_part, ]
```

Build a model on the training data and predict unseen test observations. In practice, model diagnostics and assumption checks would need to be performed before proceeding. We will use the root mean squared error (RMSE) to quantify predictive performance.

```{r prediction3}
# Train a multiple linear regression model on the training data
pred_mdl <- lm(mpg ~ cyl + wt, data = training)

# Make predictions on test set
mdl_preds <- predict(pred_mdl, newdata = test)

# Assess model performance using RMSE
RMSE(mdl_preds, test$mpg)
```

