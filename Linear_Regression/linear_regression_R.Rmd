---
title: "Linear regression using R"
author: "Sean Walsh <sw23993@my.open.ac.uk>"
date: "23 October 2018"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---

```{r setup, include=F}
# Set global options for code chunks and load required R packages
library(dplyr)
library(ggplot2)
knitr::opts_chunk$set(echo = T)
```

## Linear Regression

### Introduction

**Ordinary Least Squares (OLS) linear regression**, first published by Adrien-Marie Legendre in 1805, is a statistical technique used for the analysis and modelling of a linear relationship between a numeric response variable and one or more predictor variables (which can be continuous or categorical). Put simply, if the relationship between two variables appears to be linear, then a straight line can be fit to the data in order to model the relationship. Linear regression is used for both prediction and explanation. The regression equation (equation of a straight line) using one covariate takes the following form

$$
y = mx + c
$$

Where $y$ is the *response* variable, $m$ is the *gradient* of the line, $x$ is the value of the *covariate* and $c$ is the *intercept* on the y-axis.

The formula can also be written more formally as

$$
y_i = B_{0} + B_{1}x_{1} + e_i
$$

**Multiple linear regression** is an extension of linear regression where additional covariates are accommodated

$$
y = B_{0} + B_{1}x_{1} + B_{2}x_{2} + B_{3}x_{3} + ... + B_{p}x_{p} + e 
$$

The line of best fit is calculated in R using the base `lm()` function which also outputs the slope and intercept coefficients. The slope and intercept of a simple linear regression can also be calculated from five summary statistics: the standard deviations `sd()` of x and y, the means `mean()` of x and y, and the **Pearson correlation coefficient** `cor()` of x and y.

```{r slope and intercept, eval=F}
# How to calculate the slope in linear regression
slope <- cor(x, y) * (sd(y) / sd(x))

# How to calculate the intercept in linear regression
intercept <- mean(y) - (slope * mean(x))
```

The major **model assumptions** associated with the use of linear regression are:
  
1. A linear relationship between the response variable and the covariate(s)
2. Residuals/errors of fit are normally distributed
3. No/little multicollinearity (affects significance tests)
4. Homoscedasticity (constant variance in residuals/errors of fit)
5. Outliers do not negatively impact the regression

So, inferential procedures for linear regression are typically based on a normality assumption for the residuals. However, a second perhaps less widely known fact amongst analysts is that, as sample sizes increase, the normality assumption for the residuals is not needed. More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution. As a consequence, for moderate to large sample sizes, non-normality of residuals should not adversely affect the usual inferential procedures. This result is a consequence of an extremely important result in statistics, known as the central limit theorem.

### Exploration

The first model assumption can be quickly checked by plotting the data to see what type of relationship might exist between variables of interest. We will use the built-in `mtcars` dataset in R and visualise the relationship between fuel efficiency `mpg` and engine displacement `disp`.

```{r mpg vs disp}
# Scatterplot of mpg vs. disp
mtcars %>%
  ggplot(aes(x = disp,
             y = mpg)) +
  geom_point()
```

Upon visual inspection, the relationship appears to be linear, has a negative direction, and looks to be fairly strong. The strength of the direction can be quantified using the Pearson correlation coefficient

```{r corr 1}
# Pearson correlation coefficient
cor(mtcars$disp, 
    mtcars$mpg)

# Order of arguments does not matter
cor(mtcars$mpg,
    mtcars$disp)
```

Note that *correlation does not imply causation*; it just allows one to quantify associations. To determine causality, or at least provide evidence which supports the notion, one must typically carry out a well designed experiment (think randomised controlled trial).

If the relationship is non-linear, and linear regression is desired, a common approach is to transform the response variable and(or) predictor variable in order to coerce the relationship to one that approaches linearity. Common transformations include natural `log()`, base ten `log10()`, square root `sqrt()`, cube root and inverse. The `mpg` and `disp` relationship is already approximately linear but it can be strengthened using a square root transformation.

```{r sqrt mpg vs disp}
# Scatterplot of mpg vs. sqrt(disp)
mtcars %>%
  ggplot(aes(x = sqrt(disp), 
             y = mpg)) +
  geom_point()
```

Visually, there is not much difference but if we get the correlation once more

```{r corr 2}
# Pearson correlation coefficient
cor(sqrt(mtcars$disp), 
    mtcars$mpg)
```

### Modelling

The next step is to fit a linear regression model to the (transformed) data

```{r linear model fit}
# Fit the linear model using the formula method
linear_fit <- lm(mpg ~ sqrt(disp), data = mtcars)

# Get the model output
summary(linear_fit)
```

We now need to investigate the variance of the observed data points relative to the fitted values from the regression. The model output above gives a five-number summary of the residual distribution. The goodness of fit can be quantified with the widely used **root mean squared error (RMSE)** and the $r^2$ metric. The RMSE quantifies the variance of the model errors and is an absolute measure of fit with units identical to the response variable. The $r^2$ metric is simply the Pearson correlation coefficient, $r$, squared and quantifies the proportion of variance explained by the model.

The $t$-value is a measure of a model parameter's "signal"" strength as it is the coefficient estimate divided by its corresponding standard error ("noise"). What is the difference between the adjusted and multiple $r^2$ metrics? Adjusted $r^2$ should be used in multiple linear regression as it penalises for additional predictors which can increase $r^2$ even though there may be little association with the response variable. 

The $p$-values for the intercept and displacement terms are highly significant as is the overall model $p$-value.

```{r linear model plot}
# Plot the data and the linear model
mtcars %>%
  ggplot(aes(x = sqrt(disp), 
             y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm") + # Leave 95% CI turned on
  annotate("text",
           label = "p < 0.001",
           x = 10,
           y = 12.5,
           fontface = "italic")
```

The variance about the line of best fit looks okay. We can now check model assumptions 2 and 4. As we are dealing with just one covariate, we do not need to check model assumption 3, multicollinearity. If it were a multiple linear regression, then we would use the `cor()` function and use a threshold like 0.8 for the collinear cut-off. We could also use the **variance inflation factor (VIF)**.

Finally, outliers do not seem to be a problem in these data so we do not give model assumption 5 any further consideration.

Passing the linear model to the `plot()` function returns a series of model diagnostic plots. The first plot allows us to check the homoscedasticity assumption; no pattern is obvious here. The second plot allows us to check the normality of residual errors assumption; the points are generally close to the diagonal.

We don't need to investigate the remaining two plots but it is worth noting that the final one in the series contains useful information on the effect of any outliers that may be present.

```{r diagnostics1}
# Model diagnostics
plot(linear_fit)
```

Back to the linear model. The coefficients from the model 

```{r coeff 1}
# Get the intercept and slope
linear_fit$coefficients
```

should be identical to

```{r coeff 2}
# Formulae from earlier
slope <- cor(sqrt(mtcars$disp), mtcars$mpg) * (sd(mtcars$mpg) / sd(sqrt(mtcars$disp)))
intercept <- mean(mtcars$mpg) - (slope * mean(sqrt(mtcars$disp)))

slope; intercept
```

### Model interpretation

The $p$-value for engine displacement is highly significant and $r^2$, a measure of predictive power, is strong at 0.78. The take home message from the output is that for every unit increase in the square root of engine displacement there is a -1.28 decrease in fuel efficiency. Therefore, fuel efficiency decreases with increasing engine displacement.

### Further considerations

The number of data points is important in linear regression and can influence the $p$-value of the model. A rule of thumb for OLS linear regression is that *at least 20 data points* are required for a valid model. The $p$-value is the probability of observing data as (or more) extreme as those under investigation given a null distribution (i.e. the null hypothesis that there is no relationship between x and y). Statistical inference and $p$-value calculation will be covered in later meet-ups.

You may come across suggestions to standardise the covariates in regression models. This involves mapping the same scale ($z$-scores) to each covariate and results in the mean value of each covariate being 0. This can be useful depending on how the model is to be interpreted and communicated. When interpreting the intercept, it is described as the expected value of y when all standardised covariates are at their mean value (zero). Without standardisation, it is the expected value of y when x is equal to the absolute value 0. Standardising is not bad or wrong, it is just unnecessary most of the time.

Sometimes the literal translation of the intercept is nonsensical but we ignore it in most cases. Significance is also not always important. For example, the expected value of mpg when engine displacement is zero is 38.8. Obviously this makes no sense as there would be no engine to measure fuel efficiency from if displacement was zero cubic inches. The smallest engine displacement award actually goes to the Tata Nano (38 cubic inches).

In our linear regression model, we have a statistically significant intercept coefficient. What does this mean? It means that the intercept value is significantly different to zero. However, as mentioned earlier it cannot be taken literally as the expected mpg given an engine displacement of zero. It is a constant that is added to or subtracted from given the vector of linear predictors.


## Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression. Additional covariates are added as necessary to make a linear model (i.e. each coefficient is linearly related to its variable). Let's add some more variables to our simple model from earlier.

```{r multi-regression}
# Create a multiple linear regression model
m_linear_fit <- lm(mpg ~ sqrt(disp) + wt + cyl, data = mtcars)

# Model summary
summary(m_linear_fit)
```

Engine displacement becomes insignificant when vehicle weight and the number of cylinders are included as covariates. Multicollinearity is at play here as all three of these covariates are strongly correlated

```{r correlation matrix}
# Create the correlation matrix
cor(mtcars[c(2, 3, 6)])
```

Remove engine displacement and update the model

```{r final model}
# Create a multiple linear regression model using 0.8 as collinear cut-off
# Assess RMSE
final_linear_fit <- lm(mpg ~ cyl + wt, data = mtcars)

# Model summary
summary(final_linear_fit)
```

Now all coefficients are statistically significant and the RMSE and adjusted $r^2$ are also better than the simple model from earlier. Final model diagnostics can be carried using the `plot()` function once more

```{r diagnostics2}
# Model diagnostics
plot(final_linear_fit)
```



